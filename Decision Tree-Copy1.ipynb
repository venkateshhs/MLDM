{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input file specification :\n",
    "1. File should be CSV format (can be changed from excel to CSV)\n",
    "2. File name is my_file (.csv will come automatic if you save from excel)\n",
    "3. Don't remove the headers\n",
    "4. Remove \"S. No.\" column if present in excel file and then change to CSV\n",
    "\n",
    "5. Assumption : That the last column of csv is the target class\n",
    "6. There is no normalization in this code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for Decision Tree\n",
    "\n",
    "Predefined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(prob) :\n",
    "    ent = 0\n",
    "    for p in prob :\n",
    "        ent1 = - (p * np.log2(p))\n",
    "        ent = ent + ent1\n",
    "    return ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overall_entropy(test_data):\n",
    "    #making dictionary of every class\n",
    "    output_class = dict()\n",
    "    for every_value in test_data:\n",
    "        isPresent = every_value in output_class\n",
    "        if(isPresent == False):\n",
    "            output_class[every_value] = 1\n",
    "        else:\n",
    "            output_class[every_value] += 1\n",
    "    print(output_class)\n",
    "   \n",
    "    #calculating total\n",
    "    values = output_class.values()\n",
    "    total = sum(values)\n",
    "    print(\"Total number of data : \", total)\n",
    "    \n",
    "    #calculating entropy of each key in the dictionary\n",
    "    probability_list = []\n",
    "    for every_key in output_class.keys() :\n",
    "        val = output_class[every_key]\n",
    "        probability_list.append(val/total)\n",
    "        print(\"\\tEntropy of\",every_key,\"with probability\", val, \"/\", total, \"is :\", entropy([val/total]))\n",
    "#    print(probability_list)\n",
    "    print(\"\\033[1m\\nOverall System Entropy is : \", entropy(probability_list), \"\\033[0m\")\n",
    "    return entropy(probability_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_classification(feature_data):\n",
    "    features_dict = dict()\n",
    "    classes_dict = dict()\n",
    "    i=0\n",
    "    for feature in feature_data:\n",
    "        isPresent = feature in features_dict\n",
    "        if(isPresent == False):\n",
    "            features_dict[feature] = 1\n",
    "            child = feature + \" and \" + classes[i]\n",
    "            isPresent = child in classes_dict\n",
    "            if(isPresent == False):\n",
    "                classes_dict[child] = 1\n",
    "            else :\n",
    "                classes_dict[child] += 1\n",
    "        else:\n",
    "            features_dict[feature] += 1\n",
    "            child = feature + \" and \" + classes[i]\n",
    "            isPresent = child in classes_dict\n",
    "            if(isPresent == False):\n",
    "                classes_dict[child] = 1  \n",
    "            else :\n",
    "                classes_dict[child] += 1\n",
    "        i += 1\n",
    "    return features_dict,classes_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_impurity(features_dict,classes_dict):\n",
    "    values = features_dict.values()\n",
    "    total = sum(values)\n",
    "    \n",
    "    #calculating entropy of each key in the dictionary\n",
    "    impurity_list = []\n",
    "    for feature in features_dict.keys() :\n",
    "        prob_list = []\n",
    "        print(\"\\n\\tFor Feature\", feature, \"with total samples =\", features_dict[feature])\n",
    "        tot = features_dict[feature]\n",
    "        for classes in classes_dict.keys() :\n",
    "            if classes.startswith(feature) :\n",
    "                val = classes_dict[classes]\n",
    "                print(\"\\t\\tFor class\",classes,\":\", val,\"/\",tot)\n",
    "                prob_list.append(val/tot)\n",
    "                print(\"\\t\\tEntropy is: \",entropy([val/total]))\n",
    "        entropy_feature = entropy(prob_list)\n",
    "        print(\"\\tEntropy (sum((p*log2p) of all classes) for Feature\", feature, \"with probability\", features_dict[feature], \"/\", total, \"is :\", entropy_feature)\n",
    "        probability = features_dict[feature]/total\n",
    "        impurity_list.append(entropy_feature*probability)\n",
    "        print(\"\\tImpurity (p*entropy) for Feature\", feature, \"with probability\", features_dict[feature], \"/\", total, \"is :\", entropy_feature*probability)\n",
    "    return sum(impurity_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overall_error_rate(features_dict,classes_dict):\n",
    "    print(features_dict)\n",
    "    print(classes_dict, \"\\n\")\n",
    "    error_list = []\n",
    "    for feature in features_dict.keys() :\n",
    "        prob_list = []\n",
    "        print(feature, \"has\", features_dict[feature], \"samples\")\n",
    "        total = features_dict[feature]\n",
    "        for classes in classes_dict.keys() :\n",
    "            if classes.startswith(feature) :\n",
    "                val = classes_dict[classes]\n",
    "                prob_list.append(val)\n",
    "        highest = max(prob_list)\n",
    "        error = (total-highest)/total\n",
    "        error_list.append(error)\n",
    "        print(\"\\tError of\", feature, \"with e(t)/n(t) i.e.\", (total-highest), \"/\", total, \"is :\", error)\n",
    "    return sum(error_list)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_entropy(feature_data):\n",
    "    features_dict = dict()\n",
    "    classes_dict = dict()\n",
    "    for feature in feature_data:\n",
    "        isPresent = feature in features_dict\n",
    "        if(isPresent == False):\n",
    "            features_dict[feature] = 1\n",
    "        else:\n",
    "            features_dict[feature] += 1\n",
    "            \n",
    "    probability_list = []\n",
    "    values = features_dict.values()\n",
    "    total = sum(values)\n",
    "    for every_feature in features_dict.keys():\n",
    "        probability = features_dict[every_feature]/total\n",
    "        probability_list.append(probability)\n",
    "    return entropy(probability_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Income Married Gender Creditworthy\n",
      "0      H       Y      M            N\n",
      "1      H       N      F            Y\n",
      "2      L       Y      M            Y\n",
      "3      H       Y      M            N\n",
      "4      L       N      F            N\n",
      "5      H       Y      F            N\n",
      "6      L       Y      M            Y\n",
      "['Income', 'Married', 'Gender', 'Creditworthy']\n",
      "0    N\n",
      "1    Y\n",
      "2    Y\n",
      "3    N\n",
      "4    N\n",
      "5    N\n",
      "6    Y\n",
      "Name: Creditworthy, dtype: object\n",
      "['N' 'Y']\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"my_file.csv\", header = 0)\n",
    "print(dataset)\n",
    "Attributes = list(dataset.columns)\n",
    "print(Attributes)\n",
    "classes = dataset[Attributes[-1]]\n",
    "print(classes)\n",
    "classes_name = classes.unique()\n",
    "print(classes_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For attribute Income the classification is -->\n",
      "\n",
      "\tFor Feature H with total samples = 4\n",
      "\t\tFor class H and N : 3 / 4\n",
      "\t\tEntropy is:  0.5238824662870492\n",
      "\t\tFor class H and Y : 1 / 4\n",
      "\t\tEntropy is:  0.40105070315108626\n",
      "\tEntropy (sum((p*log2p) of all classes) for Feature H with probability 4 / 7 is : 0.8112781244591328\n",
      "\tImpurity (p*entropy) for Feature H with probability 4 / 7 is : 0.46358749969093305\n",
      "\n",
      "\tFor Feature L with total samples = 3\n",
      "\t\tFor class L and Y : 2 / 3\n",
      "\t\tEntropy is:  0.5163871205878868\n",
      "\t\tFor class L and N : 1 / 3\n",
      "\t\tEntropy is:  0.40105070315108626\n",
      "\tEntropy (sum((p*log2p) of all classes) for Feature L with probability 3 / 7 is : 0.9182958340544896\n",
      "\tImpurity (p*entropy) for Feature L with probability 3 / 7 is : 0.39355535745192405\n",
      "\n",
      "************************\n",
      "For attribute Married the classification is -->\n",
      "\n",
      "\tFor Feature Y with total samples = 5\n",
      "\t\tFor class Y and N : 3 / 5\n",
      "\t\tEntropy is:  0.5238824662870492\n",
      "\t\tFor class Y and Y : 2 / 5\n",
      "\t\tEntropy is:  0.5163871205878868\n",
      "\tEntropy (sum((p*log2p) of all classes) for Feature Y with probability 5 / 7 is : 0.9709505944546686\n",
      "\tImpurity (p*entropy) for Feature Y with probability 5 / 7 is : 0.6935361388961918\n",
      "\n",
      "\tFor Feature N with total samples = 2\n",
      "\t\tFor class N and Y : 1 / 2\n",
      "\t\tEntropy is:  0.40105070315108626\n",
      "\t\tFor class N and N : 1 / 2\n",
      "\t\tEntropy is:  0.40105070315108626\n",
      "\tEntropy (sum((p*log2p) of all classes) for Feature N with probability 2 / 7 is : 1.0\n",
      "\tImpurity (p*entropy) for Feature N with probability 2 / 7 is : 0.2857142857142857\n",
      "\n",
      "************************\n",
      "For attribute Gender the classification is -->\n",
      "\n",
      "\tFor Feature M with total samples = 4\n",
      "\t\tFor class M and N : 2 / 4\n",
      "\t\tEntropy is:  0.5163871205878868\n",
      "\t\tFor class M and Y : 2 / 4\n",
      "\t\tEntropy is:  0.5163871205878868\n",
      "\tEntropy (sum((p*log2p) of all classes) for Feature M with probability 4 / 7 is : 1.0\n",
      "\tImpurity (p*entropy) for Feature M with probability 4 / 7 is : 0.5714285714285714\n",
      "\n",
      "\tFor Feature F with total samples = 3\n",
      "\t\tFor class F and Y : 1 / 3\n",
      "\t\tEntropy is:  0.40105070315108626\n",
      "\t\tFor class F and N : 2 / 3\n",
      "\t\tEntropy is:  0.5163871205878868\n",
      "\tEntropy (sum((p*log2p) of all classes) for Feature F with probability 3 / 7 is : 0.9182958340544896\n",
      "\tImpurity (p*entropy) for Feature F with probability 3 / 7 is : 0.39355535745192405\n",
      "\n",
      "************************\n"
     ]
    }
   ],
   "source": [
    "impurity_of_attributes = dict()\n",
    "for every_attribute in Attributes[:-1]:\n",
    "    print(\"For attribute\",every_attribute, \"the classification is -->\")\n",
    "    input_value = dataset[every_attribute]\n",
    "    features_dict,classes_dict = node_classification(input_value)\n",
    "    impurity = node_impurity(features_dict,classes_dict)\n",
    "    impurity_of_attributes[every_attribute] = impurity\n",
    "    print(\"\\n************************\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall Entropy and Decrease in Impurity (Information Gain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'N': 4, 'Y': 3}\n",
      "Total number of data :  7\n",
      "\tEntropy of N with probability 4 / 7 is : 0.46134566974720237\n",
      "\tEntropy of Y with probability 3 / 7 is : 0.5238824662870492\n",
      "\u001b[1m\n",
      "Overall System Entropy is :  0.9852281360342515 \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "Root_node_entropy = overall_entropy(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Income': 0.8571428571428571, 'Married': 0.9792504246104775, 'Gender': 0.9649839288804954}\n",
      "\n",
      " 1 .For attribute Income -->  \tImpurity (Net Entropy i.i. sum of all feature values impurity) is 0.8571428571428571\n",
      "\u001b[1m\t\t\t\tDecrease in Impurity is 0.12808527889139443 \u001b[0m\n",
      "\n",
      " 2 .For attribute Married -->  \tImpurity (Net Entropy i.i. sum of all feature values impurity) is 0.9792504246104775\n",
      "\u001b[1m\t\t\t\tDecrease in Impurity is 0.0059777114237740125 \u001b[0m\n",
      "\n",
      " 3 .For attribute Gender -->  \tImpurity (Net Entropy i.i. sum of all feature values impurity) is 0.9649839288804954\n",
      "\u001b[1m\t\t\t\tDecrease in Impurity is 0.020244207153756077 \u001b[0m\n",
      "\u001b[1m\n",
      "Best split with highest decrease in impurity is Income \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(impurity_of_attributes)\n",
    "decrease_in_impurity = dict()\n",
    "n=1\n",
    "for every_impurity in impurity_of_attributes.keys():\n",
    "    print(\"\\n\",n, \".For attribute\",every_impurity, \"-->  \\tImpurity (Net Entropy i.i. sum of all feature values impurity) is\", impurity_of_attributes[every_impurity])\n",
    "    dec_impurity = Root_node_entropy - impurity_of_attributes[every_impurity]\n",
    "    print(\"\\033[1m\\t\\t\\t\\tDecrease in Impurity is\", dec_impurity, \"\\033[0m\")\n",
    "    decrease_in_impurity[every_impurity] = dec_impurity\n",
    "    n +=1\n",
    "    \n",
    "max_dec_impurity = max(decrease_in_impurity, key=decrease_in_impurity.get)\n",
    "print(\"\\033[1m\\nBest split with highest decrease in impurity is\", max_dec_impurity, \"\\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall Error Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'H': 4, 'L': 3}\n",
      "{'H and N': 3, 'H and Y': 1, 'L and Y': 2, 'L and N': 1} \n",
      "\n",
      "H has 4 samples\n",
      "\tError of H with e(t)/n(t) i.e. 1 / 4 is : 0.25\n",
      "L has 3 samples\n",
      "\tError of L with e(t)/n(t) i.e. 1 / 3 is : 0.3333333333333333\n",
      "\u001b[1m\n",
      "Overall error rate with Attributes Income as parent node =  0.5833333333333333 \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "node = (dataset[max_dec_impurity])\n",
    "features_dict,classes_dict = node_classification(node)\n",
    "Error = overall_error_rate(features_dict,classes_dict)\n",
    "print(\"\\033[1m\\nOverall error rate with Attributes\", max_dec_impurity, \"as parent node = \", Error, \"\\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gain Ratio or Normalized Impurity Decrease \n",
    "Gain Ratio of an attribute is decrease in impurity of that attribute divided by entropy of that attribute(without doing dub-sivision with class)\n",
    "GR(S,A) = Information_Gain( S,A)/ IntI(S,A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\n",
      "For attribute Income Gain ratio is 0.13000570548762885 \u001b[0m\n",
      "\u001b[1m\n",
      "For attribute Married Gain ratio is 0.006925696874193477 \u001b[0m\n",
      "\u001b[1m\n",
      "For attribute Gender Gain ratio is 0.020547735507476704 \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "for every_attribute in decrease_in_impurity.keys():\n",
    "    input_value = dataset[every_attribute]\n",
    "    node_entropy_Edt = node_entropy(input_value)\n",
    "    gain_ratio = decrease_in_impurity[every_attribute]/node_entropy_Edt\n",
    "    print(\"\\033[1m\\nFor attribute\",every_attribute , \"Gain ratio is\", gain_ratio, \"\\033[0m\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree with best split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'graphviz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-eef3a200d058>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;31m#draw tree\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mgraphviz\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m dot_data = tree.export_graphviz(clf, \n\u001b[0;32m     24\u001b[0m                                 \u001b[0mout_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'graphviz'"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#label encoding for changing string values to numerical\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "for attribute in Attributes :\n",
    "    dataset[attribute]= label_encoder.fit_transform(dataset[attribute])  \n",
    "    \n",
    "#spliting train and test data\n",
    "train_data = dataset[Attributes[:-1]]\n",
    "test_data = dataset[Attributes[-1]]\n",
    "#print(train_data)\n",
    "#print(test_data)\n",
    "\n",
    "#train classifier\n",
    "\n",
    "clf = tree.DecisionTreeClassifier(criterion='entropy')\n",
    "clf=clf.fit(train_data,test_data)\n",
    "\n",
    "#draw tree\n",
    "\n",
    "import graphviz\n",
    "dot_data = tree.export_graphviz(clf, \n",
    "                                out_file=None,\n",
    "                                feature_names=Attributes[:-1],\n",
    "                                class_names=classes_name,\n",
    "                                filled=True, \n",
    "                                rounded=True,\n",
    "                                special_characters=True)  \n",
    "graph = graphviz.Source(dot_data)\n",
    "graph"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
